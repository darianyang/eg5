Resetting modules to system default. Resetting $MODULEPATH back to system dResetting modules to system default. Resetting $MODULEPATH back BEGIN INSIDE ENV.SH

Currently Loaded Modules:
  1) cmake/3.24.2   6) impi/19.0.9          11) 
Currently Loaded Modules:
  1) cmake/3.24.2   6) impi/19.0.9          11) openmm/7.7.0_a
  2) pmix/3.2.3     7) cuda/11.3       (g)  12) swig/4.1.1_b
  3) xalt/2.10.32   8) conda/4.12.0_a       13) boost/1.72.0_a
  4) TACC           9) envwestpa/1.0_b      14) amber/22_a
  5) intel/19.1.1  10) fftw/3.3.10_a        15) westpa/22.06_b

  Where:
   g:  built for GPU

 

/work/09416/dty7/ls6/module_build_we_amber/modules/envwestpa/1.0_b/bin/python
END   INSIDE ENV.SH
starting WEST client processes on: 
c316-012.ls6.tacc.utexas.edu
current directory is /hoCUDA_VISIBLE_DEVICES =  0,1,2
-- INFO     [westpa.rc] -- loading system driver 'westpa.core.systems.WESTS-- INFO     [westpa.rc] -- loading system driver 'westpa.core.systems.WESTSystem'
-- INFO     [westpa.rc] -- Loading system options from configuration file
Updating system with the options from the configuration file
-- INFO     [westpa.rc] -- Overwriting system option: pcoord_ndim
-- INFO     [westpa.rc] -- Overwriting system option: pcoord_len
-- INFO     [westpa.rc] -- Overwriting system option: pcoord_dtype
-- INFO     [westpa.rc] -- Overwriting system option: bin_mapper
-- INFO     [westpa.rc] -- Overwriting system option: bin_target_counts
-- INFO     [westpa.core.we_driver] -- Adjust counts to exactly match target_counts: True
-- INFO     [westpa.core.we_driver] -- Obey abolute weight thresholds: True
-- INFO     [westpa.core.we_driver] -- Split threshold: 2.0
-- INFO     [westpa.core.we_driver] -- Merge cutoff: 1.0
-- INFO     [westpa.core.we_driver] -- Largest allowed weight: 0.1
-- INFO     [westpa.core.we_driver] -- Smallest allowed_weight: 1e-129
-- INFO     [westpa.work_managers.zeromq.core.ZMQWorker.7dcfabdb-3c12-4f18-b482-ee379a4485e3] -- This is ZMQWorker on c316-012.ls6.tacc.utexas.edu at PID 2931863
-- INFO     [westpa.work_managers.zeromq.core.ZMQWorker.7a9e04d4-d65c-40a3-9b7e-1da4d08cdc72] -- This is ZMQWorker on c316-012.ls6.tacc.utexas.edu at PID 2931863
-- INFO     [westpa.work_managers.zeromq.core.ZMQExecutor.3c827309-36b6-494c-8740-d87aa41198fa] -- This is ZMQExecutor on c316-012.ls6.tacc.utexas.edu at PID 2931868
-- INFO     [westpa.work_managers.zeromq.core.ZMQExecutor.1031b375-2ef7-4040-a6f3-758c29ee63b5] -- This is ZMQExecutor on c316-012.ls6.tacc.utexas.edu at PID 2931865
-- INFO     [westpa.work_managers.zeromq.core.ZMQWorker.a9e96862-6f80-46c1-8ffb-848e63ddea5f] -- This is ZMQWorker on c316-012.ls6.tacc.utexas.edu at PID 2931863
-- INFO     [westpa.work_managers.zeromq.core.ZMQExecutor.4dc29c8b-c33b-486e-8977-bda59e923-- WARNING  [westpa.work_managers.zeromq.core] -- sending SIGKILL to -- WARNING  [westpa.wor-- WARNING  [westpa.work_managers.zeromq.core] -- sending SIGKILL to-- WARNING  [westpa.work-- WARNING  [westpa.work_managers.zeromq.core] -- sending SIGKILL t-- WARNING  [westpa.work_Shutting down.  Hopefully this was on purpose?
ker process 332456
Shutting down.  Hopefully this was on purpose?
purpose?
