Resetting modules to system default. Resetting $MODULEPATH back to system default. All extra directories will be removed from $MODULEPATH.
BEGIN INSIDE ENV.SH

Currently Loaded Modules:
  1) cmake/3.24.2   6) impi/19.0.9          11) openmm/7.7.0_a
  2) pmix/3.2.3     7) cuda/11.3       (g)  12) swig/4.1.1_b
  3) xalt/2.10.32   8) conda/4.12.0_a       13) boost/1.72.0_a
  4) TACC           9) envwestpa/1.0_b      14) amber/22_a
  5) intel/19.1.1  10) fftw/3.3.10_a        15) westpa/22.06_b

  Where:
   g:  built for GPU

 

/work/09416/dty7/ls6/module_build_we_amber/modules/envwestpa/1.0_b/bin/python
END   INSIDE ENV.SH
starting WEST client processes on: 
c308-002.ls6.tacc.utexas.edu
current directory is /home1/09416/dty7/scratch/eg5/ls6-we/multi-mab_dl5_v00
environment is: 
CUDA_VISIBLE_DEVICES =  0,1,2
-- INFO     [westpa.rc] -- loading system driver 'westpa.core.systems.WESTSystem'
-- INFO     [westpa.rc] -- Loading system options from configuration file
Updating system with the options from the configuration file
-- INFO     [westpa.rc] -- Overwriting system option: pcoord_ndim
-- INFO     [westpa.rc] -- Overwriting system option: pcoord_len
-- INFO     [westpa.rc] -- Overwriting system option: pcoord_dtype
-- INFO     [westpa.rc] -- Overwriting system option: bin_mapper
-- INFO     [westpa.rc] -- Overwriting system option: bin_target_counts
-- INFO     [westpa.core.we_driver] -- Adjust counts to exactly match target_counts: True
-- INFO     [westpa.core.we_driver] -- Obey abolute weight thresholds: True
-- INFO     [westpa.core.we_driver] -- Split threshold: 2.0
-- INFO     [westpa.core.we_driver] -- Merge cutoff: 1.0
-- INFO     [westpa.core.we_driver] -- Largest allowed weight: 0.1
-- INFO     [westpa.core.we_driver] -- Smallest allowed_weight: 1e-129
-- INFO     [westpa.work_managers.zeromq.core.ZMQWorker.399fe325-0b5e-4bb2-adf3-6b826fcce9f7] -- This is ZMQWorker on c308-004.ls6.tacc.utexas.edu at PID 1804555
-- INFO     [westpa.work_managers.zeromq.core.ZMQWorker.a99fb48c-272f-47a6-a96a-b0c875bedc6f] -- This is ZMQWorker on c308-004.ls6.tacc.utexas.edu at PID 1804555
-- INFO     [westpa.work_managers.zeromq.core.ZMQExecutor.6d0f2145-13ed-4332-b24c-fa81fcab9e52] -- This is ZMQExecutor on c308-004.ls6.tacc.utexas.edu at PID 1804559
-- INFO     [westpa.work_managers.zeromq.core.ZMQExecutor.6f5749a2-bb76-465d-a660-fc2b4b87127b] -- This is ZMQExecutor on c308-004.ls6.tacc.utexas.edu at PID 1804557
-- INFO     [westpa.work_managers.zeromq.core.ZMQWorker.9090a85f-6d12-431f-9601-8f286cb40f08] -- This is ZMQWorker on c308-004.ls6.tacc.utexas.edu at PID 1804555
-- INFO     [westpa.work_managers.zeromq.core.ZMQExecutor.8d949a40-e1df-4841-9e80-21942cfa79cd] -- This is ZMQExecutor on c308-004.ls6.tacc.utexas.edu at -- WARNING  -- WARNING  [westpa.work_managers.zeromq.core] -- sending SIGKILL to worker pr-- WARNING  [w-- WARNING  [westpa.work_managers.zeromq.core] -- sending SIGKILL to worker -- WARNING  [wes-- WARNING  [westpa.work_managers.zeromq.core] -- sending SIGKILL to workeShutting down.  Hopefully this was on purpose?
s was on purpose?
worker process 1804559
-- WARNING  [westpa.work_managers.zeromq.core] -- sending SIGKILL to worker process 1804571
Shutting down.  Hopefully this was on purpose?
